在前一部分中，我们介绍了一个示例神经网络，它有一个输入、一个输出、ReLU激活函数和三个隐藏单元。现在让我们稍微推广一下，考虑有D个隐藏单元的情况，其中第d个隐藏单元是：
$$ h_d = a[θd_0 + θd_1x], \tag{3.5} $$

这些隐藏单元通过线性组合产生最终输出：
$$ y = \phi_0 + \sum_{d=1}^D \phi_d h_d, \tag{3.6} $$

浅层网络中隐藏单元的数量是网络容量的一个度量。使用ReLU激活函数时，具有D个隐藏单元的网络的输出最多有D个节点，因此是一个最多有D+1个线性区域的分段线性函数。随着我们增加更多的隐藏单元，模型可以近似更复杂的函数。

事实上，只要有足够的容量（隐藏单元），浅层网络就能以任意精度描述实线紧凑子集上定义的任何连续一维函数。要了解这一点，可以考虑每增加一个隐藏单元，就为函数增加一个线性区域。随着这些区域越来越多，它们代表的函数截面也越来越小，而这些截面越来越能被直线逼近（图 3.5）。通用逼近定理证明，对于任何连续函数，都存在一个浅层网络，可以将该函数逼近到任何指定精度。

<div style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
  <img src="/fig3.5.png" alt="图3.5" style="max-width: 100%;">
  <p style="text-align: center; font-style: italic; color: gray; font-size: 0.9em;"><strong>图 3.5 </strong>
  一维函数（虚线）通过分段线性模型的近似。a-c) 随着区域数量的增加，模型变得越来越接近连续函数。具有标量输入的神经网络每个隐藏单元创建一个额外的线性区域。这个想法可以推广到Dᵢ维度的函数。通用逼近定理证明，有足够多的隐藏单元，存在一个浅层神经网络可以描述任何在ℝᴰⁱ的紧子集上定义的给定连续函数，达到任意精度。
  </p>
</div>