在上述示例中，网络有一个单一的标量输入 $x$ 和一个单一的标量输出 $y$。然而，通用逼近定理也适用于更一般的情况，即网络将多变量输入 $x = [x_1, x_2, . . . , x_{D_i}]^T$ 映射到多变量输出预测 $y = [y_1, y_2, . . . , y_{D_o}]^T$。我们首先探讨如何扩展模型以预测多变量输出。然后我们考虑多变量输入。最后，在第3.4节中，我们提出了浅层神经网络的一般定义。

## 可视化多变量输出

要将网络扩展到多变量输出 $y$，我们只需对每个输出使用隐藏单元的不同线性函数。因此，一个具有标量输入 $x$、四个隐藏单元 $h_1$、$h_2$、$h_3$ 和 $h_4$ 以及二维多变量输出 $y = [y_1, y_2]^T$ 的网络将被定义为：
$$ \begin{aligned}
h1 &= a[θ_{10} + θ_{11}x] \\
h2 &= a[θ_{20} + θ_{21}x] \\
h3 &= a[θ_{30} + θ_{31}x] \\
h4 &= a[θ_{40} + θ_{41}x] \\
\end{aligned} \tag{3.7} $$
并且两个输出是隐藏单元的两个不同线性函数。

<div style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
  <img src="/fig3.6.png" alt="图3.6" style="max-width: 100%;">
  <p style="text-align: center; font-style: italic; color: gray; font-size: 0.9em;"><strong>图 3.6 </strong>具有一个输入、四个隐藏单元和两个输出的网络。a) 网络结构的可视化。b) 该网络产生两个分段线性函数，y1[x] 和 y2[x]。这些函数的四个“连接点”（在垂直虚线处）由于共享相同的隐藏单元，因此被限制在相同的位置，但斜率和整体高度可能有所不同。</p>
</div>

<div style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
  <img src="/fig3.7.png" alt="图3.7" style="max-width: 100%;">
  <p style="text-align: center; font-style: italic; color: gray; font-size: 0.9em;"><strong>图 3.7 </strong>具有二维多变量输入x = [x1, x2]T和标量输出y的神经网络的可视化</p>
</div>

$$ \begin{aligned}
y_1 &= ϕ_{10} + ϕ_{11}h_1 + ϕ_{12}h_2 + ϕ_{13}h_3 + ϕ_{14}h_4 \\
y_2 &= ϕ_{20} + ϕ_{21}h_1 + ϕ_{22}h_2 + ϕ_{23}h_3 + ϕ_{24}h_4 \\
\end{aligned} \tag{3.8} $$

正如我们在图3.3中所看到的，分段函数中的“连接点”取决于初始线性函数 $θ_{•0} + θ_{•1}x$ 在隐藏单元处被 ReLU 函数 $a[•]$ 截断的位置。由于两者都是输出单元，所以每个单元中的四个“连接点”必须位于相同的位置。然而，$y_1$ 和 $y_2$ 的斜率是同一四个隐藏线性区域的不同线性函数，并且整体垂直偏移可以不同（见图3.6）。

## 可视化多变量输入

为了处理多变量输入 $x$，我们扩展了输入与隐藏单元之间的线性关系。因此，一个具有两个输入 $x = [x_1, x_2]^T$ 和一个标量输出 $y$ 的网络（图3.7）可能有三个由以下定义的隐藏单元：

$$ \begin{aligned}
h_1 &= a[θ_{10} + θ_{11}x_1 + θ_{12}x_2] \\
h_2 &= a[θ_{20} + θ_{21}x_1 + θ_{22}x_2] \\
h_3 &= a[θ_{30} + θ_{31}x_1 + θ_{32}x_2] \\
\end{aligned} \tag{3.9} $$

现在每个输入都有一个斜率参数。隐藏单元以通常的方式组合形成输出：

$$ y = ϕ_0 + ϕ_1h_1 + ϕ_2h_2 + ϕ_3h_3 \tag{3.10} $$

图 3.8 阐述了这个网络的处理过程。每个隐藏单元接收两个输入的线性组合，这在 3D 输入/输出空间中形成了一个定向平面。激活函数将这些平面的负值剪切为零。然后，被剪切的平面在第二个线性函数（方程 3.10）中重新组合，以创建一个由凸多边形区域组成的连续分段线性表面（图 3.8j）。每个区域对应不同的激活模式。例如，在中央三角形区域，第一个和第三个隐藏单元处于活跃状态，第二个则处于不活跃状态。

当模型的输入超过两个时，可视化变得困难。然而，解释是相似的。输出将是输入的连续分段线性函数，其中线性区域现在是多维输入空间中的凸多面体。

请注意，随着输入维度的增加，线性区域的数量迅速增加（图3.9）。为了感受这种增长速度，可以考虑每个隐藏单元定义了一个超平面，该超平面划分了这个单元活跃的空间部分和不活跃的部分（见3.8d-f中的青色线条）。如果我们有与输入维度 $D_i$ 相同数量的隐藏单元，我们可以将每个超平面与一个坐标轴对齐（图3.10）。对于两个输入维度，这将把空间划分为四个象限。对于三个维度，这将创建八个卦限，对于 $D_i$ 个维度，这将创建 $2^{D_i}$ 个正交区域。浅层神经网络通常比输入维度有更多的隐藏单元，所以它们通常会创建超过 $2^{D_i}$ 个线性区域。

<div style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
  <img src="/fig3.8.png" alt="图3.8" style="max-width: 100%;">
  <p style="text-align: center; font-style: italic; color: gray; font-size: 0.9em;"><strong>图 3.8 </strong>在网络中处理两个输入𝐱 = [x₁, x₂]ᵀ，三个隐藏单元h1, h2, h3，和一个输出y。a-c) 每个隐藏单元的输入是两个输入的线性函数，对应于一个有向平面。亮度表示函数输出。例如，在面板(a)中，亮度代表θ₁₀ + θ₁₁x₁ + θ₁₂x₂。细线是等高线。d-f) 每个平面被ReLU激活函数截断（青线等同于图3.3d-f中的“连接点”）。g-i) 截断后的平面随后被加权，j) 并与一个偏移量一起求和，该偏移量决定了表面的整体高度。结果是一个由凸分段线性多边形区域组成的连续表面。<a href="https://udlbook.github.io/udlfigures/">交互式演示</a>）</p>
</div>
