监督学习模型是一个函数 $y = f[x, ϕ]$，它将输入 $x$ 与输出 $y$ 相关联。这种特定的关系由参数 $ϕ$ 决定。为了训练模型，我们在训练数据集 $\{x_i, y_i\}$ 上定义一个损失函数 $L[ϕ]$。这将模型预测 $f[x_i, ϕ]$ 与观察到的输出 $y_i$ 之间的不匹配量化为参数 $ϕ$ 的函数。然后我们寻找最小化损失的参数。我们在一组不同的测试数据上评估模型，以查看它对新输入的泛化能力如何。

第3至9章对这些想法进行了扩展。首先，我们来解决模型本身的问题；一维线性回归有一个明显的缺点，即它只能将输入和输出之间的关系描述为一条直线。浅层神经网络（第3章）只比线性回归稍微复杂一些，但可以描述更大范围的输入/输出关系。深度神经网络（第4章）同样具有表现力，但可以用更少的参数描述复杂函数，并且在实践中效果更好。

第5章研究不同任务的损失函数，并揭示了最小二乘损失的理论基础。第6章和第7章讨论了训练过程。第8章讨论了如何衡量模型性能。第9章考虑了正则化技术，旨在提高性能。

## 笔记

**损失函数(Loss functions)与代价函数(Cost functions)**：在机器学习的许多领域以及本书中，损失函数和代价函数这两个术语可以互换使用。然而，更准确地说，损失函数是与数据点相关的单个项（即公式2.5右侧的每个平方项），而代价函数是要最小化的整体量（即公式2.5的整个右侧）。代价函数可以包含与单个数据点无关的其他项（见9.1节）。更一般地说，目标函数是任何要最大化或最小化的函数。

**生成模型与判别模型**：
本章中的模型 $y = f[x, ϕ]$ 是判别模型。这些模型根据现实世界的测量值 $x$ 做出输出预测 $y$。另一种方法是构建一个生成模型 $x = g[y, ϕ]$，在这个模型中，现实世界的测量值 $x$ 被计算为输出 $y$ 的函数。  
生成方法有一个缺点，那就是它不直接预测 $y$。为了进行推断，我们必须将生成方程反转，即 $y = g^{-1}[x, ϕ]$，这可能很困难。然而，生成模型的优势在于我们可以内置关于数据是如何创建的先验知识。例如，如果我们想要从一张 $x$ 图像中预测一辆车的3D位置和方向 $y$，然后我们可以将关于汽车形状、三维几何和光传输的知识构建到函数 $x = g[y, ϕ]$ 中。
这似乎是个好主意，但事实上，判别模型主导了现代机器学习；利用生成模型中的先验知识所获得的优势通常被使用大量训练数据学习非常灵活的判别模型所超越。（见本章总结部分 问题2.3）

## 问题
**问题 2.1** 为了在损失函数（方程 2.5）上“下山”，我们测量其关于参数 $ϕ_0$ 和 $ϕ_1$ 的梯度。计算斜率 $∂L/∂ϕ_0$ 和 $∂L/∂ϕ_1$ 的表达式。

**问题 2.2** 展示我们可以通过将问题 2.1 中导数的表达式设为零并求解 $ϕ_0$ 和 $ϕ_1$ 来以封闭形式找到损失函数的最小值。注意，这对线性回归有效，但对更复杂的模型无效；这就是我们使用诸如梯度下降（图 2.4）这样的迭代模型拟合方法的原因。
    
**问题 2.3** 考虑将线性回归重新表述为一个生成模型，这样我们有 $x = g[y, ϕ] = ϕ_0 + ϕ_1y$。新的损失函数是什么？找到逆函数 $y = g^{-1}[x, ϕ]$ 的表达式，我们将用它来进行推断。这个模型对于给定的训练数据集 $\{x_i, y_i\}$ 会做出和判别版本相同的预测吗？建立这一点的一种方法是编写代码，使用这两种方法对三个数据点拟合一条线，并查看结果是否相同。
